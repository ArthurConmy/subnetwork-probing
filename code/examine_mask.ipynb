{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "masked_state_dict = torch.load('masked_gpt2.pt')\n",
    "N=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens.HookedTransformer import HookedTransformer\n",
    "gpt2_masked = HookedTransformer.from_pretrained(is_masked=True, model_name='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.ioi_dataset import IOIDataset\n",
    "\n",
    "ioi_dataset = IOIDataset(prompt_type=\"ABBA\", N=N, nb_templates=1)\n",
    "train_data = ioi_dataset.toks.long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff_from_ioi_dataset(\n",
    "    logits: torch.Tensor, tokens: torch.Tensor, mean=False,\n",
    "):\n",
    "    assert tokens.shape == (\n",
    "        N,\n",
    "        16,\n",
    "    ), tokens.shape  # TODO check this is not breaking things...\n",
    "    assert len(logits.shape) == 3, logits.shape\n",
    "\n",
    "    io_labels = tokens[:, 2]\n",
    "    s_labels = tokens[:, 4]\n",
    "\n",
    "    io_logits = logits[torch.arange(N), -2, io_labels]\n",
    "    s_logits = logits[torch.arange(N), -2, s_labels]\n",
    "\n",
    "    logit_diff = io_logits - s_logits\n",
    "    if mean:\n",
    "        return logit_diff.mean()\n",
    "    else:\n",
    "        return logit_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.6953, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_masked.load_state_dict(masked_state_dict)\n",
    "logit_diff_from_ioi_dataset(gpt2_masked(train_data), train_data, mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import utils\n",
    "from functools import partial \n",
    "\n",
    "def make_forward_hooks(nodes_to_mask):\n",
    "    forward_hooks = []\n",
    "    for layer in range(12):\n",
    "        for head in range(12):\n",
    "            for qkv in [\"q\", \"k\", \"v\"]:\n",
    "                mask_value = nodes_to_mask[f\"{layer}.{head}.{qkv}\"]\n",
    "                def head_ablation_hook(value, hook, head_idx, layer_idx, qkv_val, mask_value):\n",
    "                    value[:, :, head_idx, :] *= mask_value\n",
    "                    return value\n",
    "\n",
    "                a_hook = (utils.get_act_name(qkv, int(layer)), partial(head_ablation_hook, head_idx=head, layer_idx=layer, qkv_val=qkv, mask_value=mask_value))\n",
    "                forward_hooks.append(a_hook)\n",
    "    return forward_hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "new_gpt2 = HookedTransformer.from_pretrained(is_masked=False, model_name='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.4579, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_value_dict = {}\n",
    "for layer_index, layer in enumerate(gpt2_masked.blocks):\n",
    "    for head_index in range(12):\n",
    "        for q_k_v in [\"q\", \"k\", \"v\"]:\n",
    "            # total_nodes += 1\n",
    "            if q_k_v == \"q\":\n",
    "                mask_value = (\n",
    "                    layer.attn.hook_q.sample_mask()[head_index].cpu().item()\n",
    "                )\n",
    "            if q_k_v == \"k\":\n",
    "                mask_value = (\n",
    "                    layer.attn.hook_k.sample_mask()[head_index].cpu().item()\n",
    "                )\n",
    "            if q_k_v == \"v\":\n",
    "                mask_value = (\n",
    "                    layer.attn.hook_v.sample_mask()[head_index].cpu().item()\n",
    "                )\n",
    "            mask_value_dict[f\"{layer_index}.{head_index}.{q_k_v}\"] = mask_value\n",
    "\n",
    "mask_value_dict.values()\n",
    "forward_hooks = make_forward_hooks(mask_value_dict)\n",
    "logit_diff_from_ioi_dataset(new_gpt2.run_with_hooks(train_data, return_type=\"logits\", fwd_hooks=forward_hooks), train_data, mean=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ad467ea6ce724cf68e3e9460a7b5d30a577b9574f7d768503ed98371d079a52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
